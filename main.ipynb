{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "sys.path.append(\"~/Adventures/CustomA2C\")\n",
    "sys.path.append(\"~/Adventures/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from parallelEnv import parallelEnv\n",
    "from helpers import sample_trajectories\n",
    "from a2c_agent import a2c_agent\n",
    "\n",
    "agent = a2c_agent(8, 4)\n",
    "envs = parallelEnv('LunarLander-v2', n=4, seed=1234)\n",
    "device = torch.device(\"cpu\")\n",
    "print(np.arange(envs.action_space.n))\n",
    "states, actions, rewards = sample_trajectories(envs, agent, tmax=200)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.named_parameters)\n",
    "\n",
    "\n",
    "## training params ##\n",
    "ROLLOUT_LENGTH = 1000\n",
    "NUM_EPOCHS = 60\n",
    "DISCOUNT = 0.99\n",
    "BETA = 0.005 #entropy term\n",
    "CRITIC_LOSS_COEFF = 1.0 #critic term\n",
    "GRADIENT_CLIP = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "GAE_TAU = 0.95\n",
    "optimizer = torch.optim.RMSprop(agent.parameters(),lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.99 ** epoch,\n",
    "                                        last_epoch=-1,\n",
    "                                        verbose=False)\n",
    "\n",
    "\n",
    "## LOGGING SETUP ##s\n",
    "import logging\n",
    "import os\n",
    "#logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger() \n",
    "lognum = 1\n",
    "#logger.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler('./log/%s-%s.txt' % ('train', lognum))\n",
    "fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s: %(message)s'))\n",
    "fh.setLevel(logging.INFO)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "\n",
    "def learn():\n",
    "    envs.reset()\n",
    "    #take initial random steps\n",
    "    for i in range(5):\n",
    "        s, r, done, _ = envs.step(np.random.choice(np.arange(envs.action_space.n), envs.num_envs))\n",
    "        s, r, done, _ = envs.step([0]* envs.num_envs) #wait a frame\n",
    "    \n",
    "    total_timesteps = 0\n",
    "    for ep in range(NUM_EPOCHS):\n",
    "        storage = {\"states\": [], \"values\": [], \"log_probs\": [],\n",
    "                    \"actions\":[], \"entropy\":[], \"rewards\":[], \"dones_mask\":[]}\n",
    "        total_rewards = 0.0\n",
    "        \n",
    "        for t in range(ROLLOUT_LENGTH + 1): #one more step for ending!\n",
    "            # step throguh agent\n",
    "            s_tensor = torch.from_numpy(s).float().to(device)\n",
    "            preds = agent.forward(s_tensor)\n",
    "            actions = preds[\"actions\"].cpu().detach().numpy()\n",
    "\n",
    "            storage[\"states\"].append(s_tensor)\n",
    "            storage[\"values\"].append(preds[\"values\"])\n",
    "            storage[\"log_probs\"].append(preds[\"log_probs\"])\n",
    "            storage[\"actions\"].append(preds[\"actions\"])\n",
    "            storage[\"entropy\"].append(preds[\"entropy\"])\n",
    "\n",
    "            # step through env\n",
    "            s, r, done, _ = envs.step(actions)\n",
    "            storage[\"rewards\"].append(torch.tensor(r).to(device))\n",
    "            storage[\"dones_mask\"].append(torch.tensor(1-done).to(device))\n",
    "            total_rewards += r.mean()\n",
    "            total_timesteps += envs.num_envs\n",
    "\n",
    "        total_rewards /= envs.num_envs\n",
    "\n",
    "        storage[\"advantages\"] = [0]*ROLLOUT_LENGTH\n",
    "        advantage = torch.zeros((envs.num_envs, 1)).to(device)\n",
    "        ret = preds[\"values\"].detach()\n",
    "\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            td_error = storage[\"rewards\"][i] + storage[\"dones_mask\"][i]* DISCOUNT * storage[\"values\"][i+1] - storage[\"values\"][i]\n",
    "            td_error.detach()\n",
    "            advantage = advantage * GAE_TAU * DISCOUNT * storage[\"dones_mask\"][i] + td_error\n",
    "\n",
    "            storage[\"advantages\"][i] = td_error\n",
    "        \n",
    "        # collect\n",
    "        log_probs = torch.stack(storage[\"log_probs\"][:-1])\n",
    "        advantages = torch.stack(storage[\"advantages\"])\n",
    "        entropy = torch.stack(storage[\"entropy\"][:-1])\n",
    "        \n",
    "        # calculate losses\n",
    "        actor_loss = -(log_probs * advantages).mean() #this part is ascent!\n",
    "        critic_loss = 0.5 * (advantages).pow(2).mean() #this is descent\n",
    "        entropy_loss = entropy.mean()\n",
    "        L = actor_loss + CRITIC_LOSS_COEFF * critic_loss - BETA * entropy_loss\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), GRADIENT_CLIP)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"loss:\", L)\n",
    "        logger.info(\"Episode: {0:d}, loss: {1:f}, rewards: {2:f}, lr: {3:f}\".format(ep+1, L, total_rewards, optimizer.param_groups[0]['lr']))\n",
    "        if (ep+1)%20 ==0 :\n",
    "            print(\"Episode: {0:d}, loss: {1:f}, rewards: {2:f}, lr: {3:f}\".format(ep+1, L, total_rewards, optimizer.param_groups[0]['lr']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn()\n",
    "agent.save('pretraied-model-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "env = gymnasium.make('LunarLander-v2',render_mode=\"human\")\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "env.reset()\n",
    "\n",
    "agent.load('pretraied-model-1')\n",
    "\n",
    "#take initial random steps\n",
    "for i in range(5):\n",
    "    s, r, done, _, info = env.step(np.random.choice(np.arange(envs.action_space.n)))\n",
    "    s, r, done, _, info = env.step(0) #wait a frame\n",
    "\n",
    "agent.eval()\n",
    "with torch.no_grad():\n",
    "    for t in range(350):\n",
    "        # convert observations to torch\n",
    "        s = np.asarray(s)\n",
    "        s_tensor = torch.from_numpy(s).float().to(device)\n",
    "        preds = agent.forward(s_tensor)\n",
    "        actions = preds[\"actions\"].cpu().detach().numpy()\n",
    "\n",
    "        # step env\n",
    "        s, r, done,  _, info= env.step(actions)\n",
    "        if(done): break\n",
    "\n",
    "agent.train()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAC_agent import SAC\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import sys\n",
    "import metaworld\n",
    "from parallelEnv import parallelEnv, ParallelCollector\n",
    "import random\n",
    "from mtEnv import MtEnv, GymWrapper, ClsOneHotWrapper\n",
    "\n",
    "SEED=11\n",
    "random.seed(SEED)\n",
    "ENV_NAME = 'button-press-v2'\n",
    "\n",
    "def create_env_fn(env_name):\n",
    "    return TimeLimit(MtEnv(ENV_NAME), max_episode_steps=500)\n",
    "\n",
    "def create_vec_env():\n",
    "    return parallelEnv(env_fn = lambda : create_env_fn(ENV_NAME), n = 4, seed=SEED)\n",
    "\n",
    "\n",
    "\n",
    "#collector = ParallelCollector(train_mt10, env_names, 2)\n",
    "#obs, reward, terminated, truncated, info = collector.step(np.array([0.0, 0.0, 0.0, 0.0]))\n",
    "#print(obs, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = TimeLimit(MtEnv(ENV_NAME), max_episode_steps=4)\n",
    "obs, _ = train_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAC_args = {\n",
    "    \"obs_dim\" : train_env.observation_space.shape[0],\n",
    "    \"action_dim\" : train_env.action_space.shape[0],\n",
    "    \"hidden_sizes\": [8, 8],\n",
    "    \"task_fn\": lambda: TimeLimit(MtEnv(ENV_NAME), max_episode_steps=4),\n",
    "    \"max_ep_len\": 8,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_starts\": 0,\n",
    "    \"update_every\": 1,\n",
    "    \"steps_per_epoch\":2,\n",
    "    \"epochs\":3,\n",
    "    \n",
    "}\n",
    "agent = SAC(**SAC_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected 8 steps\n",
      "{'obs': tensor([[0.0105, 0.3996, 0.1994, 0.9754, 0.0133, 0.6696, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0097, 0.3997, 0.1983, 0.9885, 0.0133, 0.6696, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0133, 0.7633, 0.1150],\n",
      "        [0.0082, 0.4009, 0.2039, 0.9745, 0.0133, 0.6697, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0097, 0.4002, 0.2017, 0.9699, 0.0133, 0.6696, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0133, 0.7633, 0.1150],\n",
      "        [0.0103, 0.3997, 0.2001, 0.9694, 0.0133, 0.6696, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0105, 0.3996, 0.1994, 0.9754, 0.0133, 0.6696, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0133, 0.7633, 0.1150],\n",
      "        [0.0059, 0.3997, 0.1949, 1.0000, 0.0133, 0.6703, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0059, 0.3997, 0.1949, 1.0000, 0.0133, 0.6703, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0133, 0.7633, 0.1150]]), 'obs2': tensor([[0.0103, 0.3997, 0.2001, 0.9694, 0.0133, 0.6696, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0105, 0.3996, 0.1994, 0.9754, 0.0133, 0.6696, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0133, 0.7633, 0.1150],\n",
      "        [0.0060, 0.4014, 0.2060, 0.9875, 0.0133, 0.6697, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0082, 0.4009, 0.2039, 0.9745, 0.0133, 0.6697, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0133, 0.7633, 0.1150],\n",
      "        [0.0097, 0.4002, 0.2017, 0.9699, 0.0133, 0.6696, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0103, 0.3997, 0.2001, 0.9694, 0.0133, 0.6696, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0133, 0.7633, 0.1150],\n",
      "        [0.0065, 0.3997, 0.1954, 1.0000, 0.0133, 0.6699, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0059, 0.3997, 0.1949, 1.0000, 0.0133, 0.6703, 0.1150, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0133, 0.7633, 0.1150]]), 'actions': tensor([[-0.4901, -0.4901, -0.4901, -0.4901],\n",
      "        [ 0.5954,  0.5954,  0.5954,  0.5954],\n",
      "        [-0.0212, -0.0212, -0.0212, -0.0212],\n",
      "        [ 0.7126,  0.7126,  0.7126,  0.7126]]), 'rewards': tensor([0.2835, 0.2856, 0.2841, 0.2858]), 'dones': tensor([0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "frames = agent.collect_rollout()\n",
    "print(\"collected %d steps\" % frames)\n",
    "data = agent.replay.sample_batch(4)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mt10 = metaworld.MT10()\n",
    "env_names = list(train_mt10.train_classes.keys())\n",
    "print(env_names)\n",
    "envs=[]\n",
    "mt_cls_cnt = len(env_names)\n",
    "for i, env_name in enumerate(env_names[:4]):\n",
    "    env_cls = train_mt10.train_classes[env_name]\n",
    "    task_list = [task for task in train_mt10.train_tasks if task.env_name == env_name]\n",
    "\n",
    "    env = env_cls()      \n",
    "    task = random.choice(task_list)\n",
    "    env.set_task(task)\n",
    "    env = GymWrapper(env)\n",
    "    env = TimeLimit(env, 500)\n",
    "    env = ClsOneHotWrapper(env, i, 4)\n",
    "    envs.append(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(4): \n",
    "    envs[i].reset()\n",
    "    print(envs[i].step(np.array([1.0]*4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
